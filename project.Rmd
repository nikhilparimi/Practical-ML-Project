---
title: "Predicting the manner of exercise using HAR dataset"
author: "Nikhil Parimi"
date: "2024-12-15"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

Using a dataset provided by HAR <http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har> we aim to build a predictive model(s) to accurately predict the "classe" variable.

This will be done as follows:
- Processing the data
- Data cleaning
- Exploratory data analysis
- Model selection
- Make predictions

```{r packages, results = 'hide'}
library(caret)
library(dplyr)
library(corrplot)
```

## Processing data

Load the data from the data directory

```{r}
training_raw <- read.csv("data/pml-training.csv")
testing_raw <- read.csv("data/pml-testing.csv")
dim(training_raw)
dim(testing_raw)
```

Here we can see our training data has 160 columns and 19622 rows, by exploring the data let's see how we can clean the data further.

```{r}
#head(training_raw, n = 2)
#summary(training_raw)
```

## Data Cleaning

First we shall check which columns have NA values and see how many NA values each contain.

```{r}
na_cols <- colSums(is.na(training_raw)) != 0
only_na_cols <- colSums(is.na(training_raw[,na_cols]))
summary(only_na_cols)
```

Here we can see that of the columns that contain NA values, all of them contain **19216** NA values out of **19622** total observations. That means **97.9%** of these columns are filled with NA values, this would be redundant to include in the model so these columns will be removed from both the training and testing set.

```{r}
training_1 <- training_raw[,!na_cols]
testing_1 <- testing_raw[,!na_cols]
```

Of the remaining columns we shall check for empty values.

```{r}
empty_cols <- colSums(training_1=="") != 0
only_empty_cols <- colSums(training_1[,empty_cols]=="")
summary(only_empty_cols)
```

Here we measure a similar percentage of values (**97.9%**) of observations in a column are missing, so we can safely remove these columns as well.

```{r}
training_2 <- training_1[,!empty_cols]
testing_2 <- testing_1[,!empty_cols]
```

After viewing the data in a bit more detail we notice that the first 7 columns are metadata that is irrelevant to the outcome and so will be removed.

```{r}
training_3<- training_2[,-c(1:7)]
testing_3<- testing_2[,-c(1:7)]
```

Finally we will check for any columns with near zero value variances.

```{r}
nzv <- nearZeroVar(training_3, saveMetrics = TRUE)
nzv$nzv
```
Here we can see that none of the columns have near zero / zero variance, so we shall use training_3 and testing 3 as our cleaned data after converting the "classe" variable to a factor.

```{r}
training_3$classe <- as.factor(training_3$classe)
training_cleaned <- training_3
testing_cleaned <- testing_3
dim(training_cleaned)
dim(testing_cleaned)
```

## Exploratory data analysis

We shall now split the data into a training set and a validation set due to the testing set being the ultimate goal of what we would like to predict.

```{r}
set.seed(50)

inTrain <- createDataPartition(training_cleaned$classe, p=0.7, list=FALSE)
training_set <- training_cleaned[inTrain,]
validation_set <- training_cleaned[-inTrain,]
classe_index <- length(names(training_set))
```

Testing and plotting the correlation between variables including the predictor variable "classe"

```{r}
temp_set <- training_set
temp_set$classe <- as.numeric(temp_set$classe)
correlations <- cor(temp_set)

corrplot(correlations, method = "color", type = "lower", tl.cex = 0.6, tl.col="black")
```

In the bottom row we can see that none of the variables have a **strong** correlation with the predictor "classe" variable (as evidenced by the softer colors), whilst there appears to be stronger correlations between some variables. We can see this in code below:

```{r}
# Correlations contains the information in the matrix above, it suffices to simply look at the final row of the column to analyse the correlations between parameters.
dim(correlations)
# Taking the 53rd row and excluding the last column
corr_with_classe <- correlations[53,]
corr_with_classe <- corr_with_classe[-53]
ordered_indices <- order(abs(corr_with_classe), decreasing = TRUE)
ordered_corr <- corr_with_classe[ordered_indices]
head(ordered_corr, n = 5)

```

The variable with the strongest correlation to "classe" appears to be "pitch_forearm" which only has a magnitude of 0.356, which is quite poor relationship between the two variables.

## Model Selection

The 3 models I will choose to train are Random Forest, Gradient Boosted Trees and Support Vector Machines. Between these 3 models, I will train each of the models using k-fold cross validation, where k = 3 and also perform pre-processing of PCA, with a threshold of 0.99 (Keep 99% of the variance). We will also set tuneLength = 3 or 5 for the models, which will also tune the parameters for each model between an evenly spaced range of values.

```{r}
# Set up cross-validation control with k=3 folds and PCA pre-processing
control <- trainControl(
  method = "cv",
  number = 3,
  preProcOptions = list(thresh = 0.99)
)
```

### Model 1: Random Forest

```{r random_forest, warning=FALSE}
fit_rf <- train(classe ~ .,
                data = training_set,
                method = "rf",
                trControl = control,
                preProcess="pca",
                tuneLength = 3)
# Predictions on validation set and training set
pred_rf <- predict(fit_rf, validation_set)
pred_train_rf <- predict(fit_rf, training_set)

#Confusion matrices for validation set and training set
cm_rf <- confusionMatrix(pred_rf, validation_set$classe)
cm_train_rf <- confusionMatrix(pred_train_rf, training_set$classe)
cm_rf
```

### Model 2: Gradient Boosted Trees
```{r gradient_boosted_tree, warning=FALSE}
fit_gbm <- train(classe ~ .,
                data = training_set,
                method = "gbm",
                trControl = control,
                preProcess="pca",
                tuneLength = 5,
                verbose = FALSE)

# Predictions on validation set and training set
pred_gbm <- predict(fit_gbm, validation_set)
pred_train_gbm <- predict(fit_gbm, training_set)

#Confusion matrices for validation set and training set
cm_gbm <- confusionMatrix(pred_gbm, validation_set$classe)
cm_train_gbm <- confusionMatrix(pred_train_gbm, training_set$classe)
cm_gbm
```

### Model 3: Support Vector Machines
```{r support_vector_machines, warning=FALSE}
fit_svm <- train(classe ~ .,
                data = training_set,
                method = "svmLinear",
                trControl = control,
                preProcess="pca",
                tuneLength = 5,
                verbose = FALSE)
# Predictions on validation set and training set
pred_svm <- predict(fit_svm, validation_set)
pred_train_svm <- predict(fit_svm, training_set)

#Confusion matrices for validation set and training set
cm_svm <- confusionMatrix(pred_svm, validation_set$classe)
cm_train_svm <- confusionMatrix(pred_train_svm, training_set$classe)
cm_svm
```

### Comparing the models:
Printed below is a summary of the accuracy of the 3 models on the training set and the validation (test) set as well as the out of sample error.

```{r, warning=FALSE}
models <- c("RF", "GBM", "SVM")
train_acc <- round(c(cm_train_rf$overall[1], cm_train_gbm$overall[1], cm_train_svm$overall[1]),3)
test_acc <- round(c(cm_rf$overall[1], cm_gbm$overall[1], cm_svm$overall[1]),3)
oos_error <- 1 - test_acc

data.frame(train_acc = train_acc, test_acc = test_acc, oos_error = oos_error, row.names = models)
```

From the results above the best model is the Random Forest model with a test accuracy of 0.985 and an out of sample error rate of 0.015. Although the training accuracy of 1.0 may be cause for concern as it may imply over fitting, the model generalises well to data it hasn't seen before, as evidenced by the validation (test) set accuracy rate.

## Predictions

In the code block below, we will run our Random Forest model on the test set (the set with unknown "classe" variable) to see the predictions we get.

```{r predictions}
# Exclude the final column (problem_id) from the testing_set
pred_final <- predict(fit_rf, testing_cleaned[,-length(names(testing_cleaned))])
pred_final
```

## Appendix
Plotting the models selected using cross validation (k=3) and tuneLength (3 / 5).

### 1.Random Forest tuning
```{r rf}
plot(fit_rf)
```

### 2.Gradient Boosted Trees tuning
```{r gbm}
plot(fit_gbm)
```
